{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a98c55-901c-4fa7-b093-cc0deb4d58fc",
   "metadata": {},
   "source": [
    "# Reduce catalog based on SEDs\n",
    "## January 2024\n",
    "### Bethany Ludwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54991373-feeb-4335-a823-1348da797da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import os, glob\n",
    "import matplotlib\n",
    "from astropy.io import ascii\n",
    "from matplotlib import rc\n",
    "from scipy import stats\n",
    "from astropy.coordinates import SkyCoord\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# Set dir depending on machine \n",
    "directory = '/home/bethany/Projects/0_Data/0_SUMS_Catalogs/CandidateCatalog/'\n",
    "# directory = '/Volumes/Untitled/0_Data/SUMS_Catalogs/Candidates/'\n",
    "synth_photom_file = '/home/bethany/Projects/0_Data/1_Models/Stripped_Stars/photometry_CMFGEN_composites.txt'\n",
    "# synth_photom_file = '/Volumes/Untitled/0_Data/Models/photometry_CMFGEN_composites.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20822f3f-ae85-42bd-a673-46de01069aba",
   "metadata": {},
   "source": [
    "## Load in photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c04a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "galaxy\n",
      "lmc    8906\n",
      "smc    4447\n",
      "Name: count, dtype: int64\n",
      "How many stars we have:  13353\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "# Read in magnitude reduced files \n",
    "df = pd.read_csv(directory+'1_magnitude_reduced.csv') \n",
    "print(df.galaxy.value_counts())\n",
    "\n",
    "# Make a key\n",
    "df['key'] = np.arange(df.shape[0])\n",
    "\n",
    "# Rename filters for simplicity, we can rename them back at the end\n",
    "df = df.rename(columns={'uvw2_dered':'w2','uvm2_dered':'m2','uvw1_dered':'w1',\n",
    "                                          'U_dered':'u','B_dered':'b','V_dered':'v','I_dered':'i',\n",
    "                                          'uvw2 - b':'w2 - b', 'uvw2 - v':'w2 - v', 'uvw2 - i':'w2 - i',\n",
    "                                          'uvw1 - b':'w1 - b', 'uvw1 - v':'w1 - v', 'uvw1 - i':'w1 - i', \n",
    "                                          'uvm2 - b':'m2 - b', 'uvm2 - v':'m2 - v', 'uvm2 - i':'m2 - i'})\n",
    "\n",
    "# Select keys\n",
    "optical_columns = ['u','b','v','i']\n",
    "uv_columns = ['w2','m2','w1']\n",
    "color_columns = ['w2 - b', 'w2 - v', 'w2 - i',\n",
    "                'm2 - b', 'm2 - v', 'm2 - i',\n",
    "                'w1 - b', 'w1 - v', 'w1 - i']\n",
    "\n",
    "# Make sure we have all the discovery stars except Star 15 (and now Star 19) which we lost in the last notebook \n",
    "def check_stars(df):\n",
    "    # Make sure we have all the discovery stars \n",
    "    discovery_names = ['Star_1','Star_2','Star_3','Star_4','Star_5','Star_6','Star_7','Star_8','Star_9','Star_10','Star_11','Star_12','Star_13','Star_14','Star_16','Star_17','Star_18','Star_20','Star_21','Star_22','Star_23','Star_24','Star_25']\n",
    "    c = 0\n",
    "    for star in discovery_names:\n",
    "        if star not in df.discovery_name.unique():\n",
    "            print (star)\n",
    "            c += 1\n",
    "    if c == 0:\n",
    "        print (\"All Remaining Discovery Stars After Magnitude Reductions Have Been Found\")\n",
    "\n",
    "# Print how many stars we have\n",
    "print(\"How many stars we have: \",df.shape[0])\n",
    "\n",
    "# Read in synthetic photometry\n",
    "names = ['Minit_strip','M_MS','frac_MS','U','B','V','R','I','UVM2','UVW2','UVW1']\n",
    "\n",
    "sf = pd.read_csv(synth_photom_file,comment='#',delimiter='\\t',names=names)\n",
    "\n",
    "# Make dictionary of the minimum and maximum difference between different filters\n",
    "max_mag_err = 0.217\n",
    "min_diff = {'w2 - m2': np.min(sf.UVW2-sf.UVM2) - max_mag_err,\n",
    "            'm2 - w1': np.min(sf.UVM2-sf.UVW1) - max_mag_err,\n",
    "            'w1 - u': np.min(sf.UVW1-sf.U) - max_mag_err,\n",
    "            'u - b': np.min(sf.U-sf.B) - max_mag_err,\n",
    "            'b - v': np.min(sf.B-sf.V) - max_mag_err,\n",
    "            'v - i': np.min(sf.V-sf.I) - max_mag_err,\n",
    "            # Additional UV - Optical Filters\n",
    "            'w1 - b': np.min(sf.UVW1-sf.B) - max_mag_err,\n",
    "            'm2 - u': np.min(sf.UVM2-sf.U) - max_mag_err,\n",
    "            'm2 - b': np.min(sf.UVM2-sf.B) - max_mag_err,\n",
    "           }\n",
    "\n",
    "max_diff = {'w2 - m2': np.max(sf.UVW2-sf.UVM2) + max_mag_err,\n",
    "            'm2 - w1': np.max(sf.UVM2-sf.UVW1) + max_mag_err,\n",
    "            'w1 - u': np.max(sf.UVW1-sf.U) + max_mag_err,\n",
    "            'u - b': np.max(sf.U-sf.B) + max_mag_err,\n",
    "            'b - v': np.max(sf.B-sf.V) + max_mag_err,\n",
    "            'v - i': np.max(sf.V-sf.I) + max_mag_err,\n",
    "            # Additional UV - Optical Filters\n",
    "            'w1 - b': np.max(sf.UVW1-sf.B) + max_mag_err,\n",
    "            'm2 - u': np.max(sf.UVM2-sf.U) + max_mag_err,\n",
    "            'm2 - b': np.max(sf.UVM2-sf.B) + max_mag_err,\n",
    "           }\n",
    "\n",
    "check_stars(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958b46e-68fe-4e23-b99c-07f6437b1904",
   "metadata": {},
   "source": [
    "#### If blue due to one filter, check that that filter is not super different from adjacent filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4640d35d-667f-4ca8-bc20-55ca1aa5530f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solving m2\n",
      "Checking Colors: ['m2 - b', 'm2 - v', 'm2 - i']\n",
      "Calculating difference for: \"w2 - m2\"\n",
      "Calculating difference for: \"m2 - w1\"\n",
      "\n",
      "Solving w1\n",
      "Checking Colors: ['w1 - b', 'w1 - v', 'w1 - i']\n",
      "Calculating difference for: \"m2 - w1\"\n",
      "Calculating difference for: \"w1 - u\"\n",
      "\n",
      "Solving b\n",
      "Checking Colors: ['w2 - b', 'm2 - b', 'w1 - b']\n",
      "Calculating difference for: \"u - b\"\n",
      "Calculating difference for: \"b - v\"\n",
      "\n",
      "Solving v\n",
      "Checking Colors: ['w2 - v', 'm2 - v', 'w1 - v']\n",
      "Calculating difference for: \"b - v\"\n",
      "Calculating difference for: \"v - i\"\n",
      "\n",
      "How many we have:  11129\n",
      "How many we lost:  2224\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "size = df.shape[0]\n",
    "# Get rows where only one filter made it blue\n",
    "blue_in_one = df[df['n_blue'] <= 3].copy()\n",
    "\n",
    "# Compare adjacent mags to see if the filter is super different from the sed \n",
    "bad_filter_index = []\n",
    "\n",
    "# For the filters on the ends, uvw2 and i, you only need to compare one color\n",
    "filter_1 = ['w2','v']\n",
    "filter_2 = ['m2','i']\n",
    "filter_color = ['w2','i']\n",
    "\n",
    "for f1, f2, fc in zip(filter_1,filter_2,filter_color):\n",
    "    # Get all color combos for a specific filter \n",
    "    # For example, get all the w2 - b, w2 - v, w2 - i for w2\n",
    "    c = list(filter(lambda x: fc in x,color_columns))\n",
    "    # Get everything where the respective filter is blue \n",
    "    blue = blue_in_one[(blue_in_one[c[0]] == 'blue') & (blue_in_one[c[1]] == 'blue') & (blue_in_one[c[2]] == 'blue')]\n",
    "    # Calculate the difference between the filter and its adjacent filter\n",
    "    diff = blue[f'{f1}'] - blue[f'{f2}']\n",
    "    # Compare to synthetic photometry      \n",
    "    bad = blue[(diff > max_diff[f'{f1} - {f2}']) | (diff < min_diff[f'{f1} - {f2}']) ]\n",
    "    bad_keys = bad['key'].values\n",
    "    print('')\n",
    "    bad_filter_index.append(bad_keys)\n",
    "\n",
    "# For all the other filters you need to compare whats on the left and the right \n",
    "filters = ['w2','m2','w1','u','b','v','i']          \n",
    "\n",
    "for i in np.arange(6):\n",
    "    # Skip the ends\n",
    "    if filters[i] == 'w2' or filters[i] == 'i' or filters[i] == 'u':\n",
    "        continue\n",
    "    # The filter you're considering\n",
    "    f0 = filters[i]\n",
    "    # The previous filter\n",
    "    f1 = filters[i-1]\n",
    "    # The next filter\n",
    "    f2 = filters[i+1]\n",
    "    print(f'Solving {f0}')\n",
    "    # Get colors \n",
    "    c = list(filter(lambda x: f0 in x,color_columns))\n",
    "    print(f'Checking Colors: {c}')\n",
    "    # Get everything where the respective filter is blue \n",
    "    blue = blue_in_one[(blue_in_one[c[0]] == 'blue') & (blue_in_one[c[1]] == 'blue') & (blue_in_one[c[2]] == 'blue')] \n",
    "    # Calculate differences\n",
    "    print(f'Calculating difference for: \"{f1} - {f0}\"')\n",
    "    diff_1 = blue[f'{f1}'] - blue[f'{f0}']\n",
    "    print(f'Calculating difference for: \"{f0} - {f2}\"')\n",
    "    diff_2 = blue[f'{f0}'] - blue[f'{f2}']\n",
    "    # Calculate synthetic differences\n",
    "    synth_1 = [min_diff[f'{f1} - {f0}'], max_diff[f'{f1} - {f0}']]\n",
    "    synth_2 = [min_diff[f'{f0} - {f2}'], max_diff[f'{f0} - {f2}']]\n",
    "    # Compare                    The Left                                        The Right\n",
    "    bad = blue[(diff_1 < synth_1[0]) | (diff_1 > synth_1[1]) | (diff_2 < synth_2[0]) | (diff_2 > synth_2[1])]\n",
    "    bad_keys = bad['key'].values\n",
    "    if len(bad_keys) == 0: \n",
    "        print('Check Filter')\n",
    "        print(f0)\n",
    "    print('')\n",
    "    # Append\n",
    "    bad_filter_index.append(bad_keys)\n",
    "    \n",
    "# Flatten the index\n",
    "bad_filter_index = [item for row in bad_filter_index for item in row]\n",
    "# Sort the index \n",
    "bad_filter_index = np.sort(bad_filter_index)\n",
    "\n",
    "df = df[~df['key'].isin(bad_filter_index)]\n",
    "\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many we lost: ',size-df.shape[0])\n",
    "check_stars(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d29cd-895d-41d6-8eb7-3d4ecdadfa48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove sources that have a uv-optical mismatch \n",
    "There's a sharp enough discontinuity between adjacent(ish) uv and optical filters that we think it is not the same source. Using synthetic photometry Ylva created we determine what the max mag diff between UV and Optical filters should be and restrict based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcc7c40-ed2b-4c7f-85fc-2f8c7b3ad877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many we have:  10829\n",
      "How many we lost:  300\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "size = df.shape[0]\n",
    "# Mismatch #1: UV and Optical components of SED are fairly flat but there's a large discrepency between them\n",
    "\n",
    "# If all the mags are within the max allowed error, we consider it 'flat'\n",
    "flat_std = max_mag_err\n",
    "flat_mag_jump = 1. + max_mag_err\n",
    "\n",
    "flat_jump_index = []\n",
    "\n",
    "for ind,row in df.iterrows():\n",
    "    uv = row[uv_columns].values\n",
    "    opt = row[optical_columns].values\n",
    "    \n",
    "    if np.nanstd(uv) < flat_std and np.nanstd(opt) < flat_std:\n",
    "        # If you have a flat sed look for a big jump \n",
    "        if np.abs(np.nanmean(uv) - np.nanmean(opt)) > flat_mag_jump:\n",
    "            flat_jump_index.append(row['key'])\n",
    "            \n",
    "# Remove from sample \n",
    "df = df[~df['key'].isin(flat_jump_index)]\n",
    "\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many we lost: ',size - df.shape[0])\n",
    "check_stars(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc0e824-d5a1-4ade-ae51-e60a4dadea7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many we have:  9645\n",
      "How many were categorized as mismatched:  1184\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "size = df.shape[0]\n",
    "# Mismatch #2: Check either UVW1 or UVW2 against U or B to see if there's a large jump \n",
    "def not_in_bin(row,uv_filter,optical_filter):\n",
    "    diff = row[uv_filter] - row[optical_filter]\n",
    "    if diff < min_diff[f'{uv_filter} - {optical_filter}'] or diff > max_diff[f'{uv_filter} - {optical_filter}']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def exists(row,Filter):\n",
    "    return np.isfinite(row[Filter])\n",
    "\n",
    "mismatch_index = []\n",
    "\n",
    "for i,r in df.iterrows():\n",
    "\n",
    "    # Ideal case: UVW1, U, and B, all exist. \n",
    "    if exists(r,'w1') and exists(r,'u') and exists(r,'b'):\n",
    "        # If UVW1 - U and UVW1 - B are both not in synthetic bin then drop\n",
    "        if not_in_bin(r,'w1','u') and not_in_bin(r,'w1','b'):\n",
    "            mismatch_index.append(r['key'])\n",
    "    \n",
    "    # UVW1 and U exist, but B does not.\n",
    "    if exists(r,'w1') and exists(r,'u') and not exists(r,'b'):\n",
    "        # If UVW1 - U is not in synthetic bin then drop\n",
    "        if not_in_bin(r,'w1','u'):\n",
    "            mismatch_index.append(r['key'])\n",
    "            \n",
    "    # UVW1 and B exist, but U does not. \n",
    "    if exists(r,'w1') and exists(r,'b') and not exists(r,'u'):\n",
    "        # If UVW1 - B is not in synthetic bin then drop\n",
    "        if not_in_bin(r,'w1','b'):\n",
    "            mismatch_index.append(r['key'])    \n",
    "    \n",
    "    # UVM2, U, and B, exist, but UVW1 does not. \n",
    "    if exists(r,'m2') and exists(r,'u') and exists(r,'b') and not exists(r,'w1'):\n",
    "        # If UVM2 - U and UVM2 - B are both not in synthetic bin then drop\n",
    "        if not_in_bin(r,'m2','u') and not_in_bin(r,'m2','b'):\n",
    "            mismatch_index.append(r['key']) \n",
    "            \n",
    "    # UVM2 and U exist, but UVW1 and B do not.\n",
    "    if exists(r,'m2') and exists(r,'u') and not exists(r,'w1') and not exists(r,'b'):\n",
    "        # If UVM2 - U is not in synthetic bin then drop\n",
    "        if not_in_bin(r,'m2','u'):\n",
    "            mismatch_index.append(r['key'])     \n",
    "\n",
    "    # UVM2 and B exist, but UVW1 and U do not. \n",
    "    if exists(r,'m2') and exists(r,'b') and not exists(r,'w1') and not exists(r,'u'):\n",
    "        # If UVM2 - B is not in synthetic bin then drop\n",
    "        if not_in_bin(r,'m2','b'):\n",
    "            mismatch_index.append(r['key'])    \n",
    "            \n",
    "df = df[~df['key'].isin(mismatch_index)]\n",
    "\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many were categorized as mismatched: ',size - df.shape[0])\n",
    "check_stars(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e63a28-baf5-4584-a7ff-b68d84504c52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove sources that weren't classified as a mismatch but do increase in red filters.\n",
    "What are these guys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75e94ae-f0c5-4de2-a3cb-6ffc74a67f80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many we have:  6686\n",
      "How many we lost:  2959\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "size = df.shape[0]\n",
    "red_index = []\n",
    "\n",
    "for ind, row in df.iterrows():\n",
    "    y = np.array(row[optical_columns].values).astype(float)\n",
    "    # Calculate derivatives\n",
    "    d = np.diff(y)\n",
    "    # If derivatives are negative 2 or more times (indicating positive slope for mags) then save\n",
    "    n = len(d[np.isfinite(d) & (d<0)])\n",
    "    if n >= 2:\n",
    "        red_index.append(row['key'])\n",
    "\n",
    "df = df[~df['key'].isin(red_index)].reset_index(drop=True)\n",
    "\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many we lost: ',size-df.shape[0])\n",
    "check_stars(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dbbbe-0043-4367-b560-4ff5776898d3",
   "metadata": {},
   "source": [
    "#### Remove sources with bumpy SEDs\n",
    "Using synthetic photometry Ylva created we determine what the max mag diff between adjacent filters should be and restrict based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e65673-e5ec-491b-b346-f93b0ba308c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many we have:  6217\n",
      "How many we lost:  469\n",
      "All Remaining Discovery Stars After Magnitude Reductions Have Been Found\n"
     ]
    }
   ],
   "source": [
    "vf = df.copy()\n",
    "\n",
    "def not_in_bin(row,uv_filter,optical_filter):\n",
    "    diff = row[uv_filter] - row[optical_filter]\n",
    "    if diff < min_diff[f'{uv_filter} - {optical_filter}'] or diff > max_diff[f'{uv_filter} - {optical_filter}']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "variable_index = []\n",
    "filters = ['w2','m2','w1','u','b','v','i']\n",
    "for ind,row in vf.iterrows():\n",
    "    # Get the sed\n",
    "    sed = row[filters]\n",
    "    # Take the derivatives         \n",
    "    div = np.abs(np.diff(sed))\n",
    "    # Compare to allowed differences \n",
    "    compare = [not_in_bin(row,f1,f2) for f1,f2 in zip(filters[:-1],filters[1:])]\n",
    "    # Where do the bumps occur? \n",
    "    loc = np.where(compare)[0]\n",
    "    # How many bumps are there?\n",
    "    n_bumps = len(loc)\n",
    "    if n_bumps == 0: \n",
    "        continue\n",
    "    # If there are three or more bumps it's automatically variable \n",
    "    if n_bumps >= 3:\n",
    "        variable_index.append(row.key)\n",
    "        \n",
    "    # The location of the bumps tell us which filters are involved \n",
    "    # Zero out colors related to that filter and see if it is still blue \n",
    "    # For instance: W2 - M2 is the first element in the div array\n",
    "        \n",
    "    # If there are two bumps and they are next to each other: \n",
    "    if n_bumps == 2 and (loc[1] == loc[0] - 1 or loc[1] == loc[0] + 1): \n",
    "        # UVM2 is connected\n",
    "        if loc[0] == 0 and loc[1] == 1:\n",
    "            vf.loc[vf.key==row.key,color_columns[1]] = np.nan\n",
    "        # UVW1 is connected\n",
    "        if loc[0] == 1 and loc[1] == 2:\n",
    "            vf.loc[vf.key==row.key,color_columns[2]] = np.nan\n",
    "        # B is connected, zero out related colors\n",
    "        if loc[0] == 3 and loc[1] == 4:\n",
    "            vf.loc[vf.key==row.key,color_columns[4]] = np.nan\n",
    "        # V is connected, zero out related colors\n",
    "        if loc[0] == 4 and loc[1] == 5:\n",
    "            vf.loc[vf.key==row.key,color_columns[5]] = np.nan\n",
    "    # If there is one bump or they are not connected\n",
    "    else:\n",
    "        # UVW2 and UVM2 \n",
    "        if 0 in loc:\n",
    "            vf.loc[vf.key==row.key,color_columns[0] + color_columns[1]] = np.nan\n",
    "        # UVM2 and UVW1 \n",
    "        if 1 in loc: \n",
    "            vf.loc[vf.key==row.key,color_columns[1] + color_columns[2]] = np.nan\n",
    "        # UVW1 and U (we don't color by U)\n",
    "        if 2 in loc: \n",
    "            vf.loc[vf.key==row.key,color_columns[2]] = np.nan\n",
    "        # U and B (we don't color by U)\n",
    "        if 3 in loc: \n",
    "            vf.loc[vf.key==row.key,color_columns[4]] = np.nan\n",
    "        # B and V \n",
    "        if 4 in loc: \n",
    "            vf.loc[vf.key==row.key,color_columns[4] + color_columns[5]] = np.nan\n",
    "        # V and I \n",
    "        if 5 in loc: \n",
    "            vf.loc[vf.key==row.key,color_columns[5] + color_columns[6]] = np.nan\n",
    "            \n",
    "# Recalculate to see what is still blue \n",
    "n_blue = vf[color_columns].isin(['blue']).sum(axis=1)\n",
    "\n",
    "# Find out which rows are no longer blue \n",
    "nolongerblue = vf.loc[n_blue == 0,'key']\n",
    "\n",
    "# Combine Variable Index and No Longer Blue, Remove Duplicates\n",
    "variable_index = np.sort(np.unique(np.append(variable_index,nolongerblue)))\n",
    "\n",
    "# Drop variables\n",
    "df = df[~df['key'].isin(variable_index)]\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many we lost: ',len(variable_index))\n",
    "check_stars(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c87966-fddf-4e21-a5e1-66fd2940732c",
   "metadata": {},
   "source": [
    "#### For what's left, remove 'I' and calculate to see if it still blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f039e26-6102-4b5e-8cc1-e50d1254f7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many we have:  4649\n",
      "How many we lost:  1568\n",
      "Star_17\n",
      "Star_25\n"
     ]
    }
   ],
   "source": [
    "# Change all color combos with 'i' to not be blue. \n",
    "i_df = df.copy()\n",
    "\n",
    "for color in ['w2 - i','w1 - i', 'm2 - i']:\n",
    "    i_df[color] = i_df[color].replace('blue',np.nan)\n",
    "\n",
    "# Recalculate to see what is still blue \n",
    "n_blue = i_df[color_columns].isin(['blue']).sum(axis=1)\n",
    "\n",
    "# Find out which rows are no longer blue \n",
    "bad_i_index = i_df.loc[n_blue == 0,'key']\n",
    "\n",
    "# Drop variables\n",
    "df = df[~df['key'].isin(bad_i_index)]\n",
    "print(f'How many we have: ',df.shape[0])\n",
    "print(f'How many we lost: ',len(bad_i_index))\n",
    "check_stars(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d481db",
   "metadata": {},
   "source": [
    "#### Recalculate n_bumps for everything in mag_reduced file so that we can save it to use for quality cuts later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2409d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reread in and calculate the number of jumps for everything: \n",
    "df = pd.read_csv(directory+'1_magnitude_reduced.csv') \n",
    "\n",
    "# Rename filters for simplicity, we can rename them back at the end\n",
    "df = df.rename(columns={'uvw2_dered':'w2','uvm2_dered':'m2','uvw1_dered':'w1',\n",
    "                                          'U_dered':'u','B_dered':'b','V_dered':'v','I_dered':'i',\n",
    "                                          'uvw2 - b':'w2 - b', 'uvw2 - v':'w2 - v', 'uvw2 - i':'w2 - i',\n",
    "                                          'uvw1 - b':'w1 - b', 'uvw1 - v':'w1 - v', 'uvw1 - i':'w1 - i', \n",
    "                                          'uvm2 - b':'m2 - b', 'uvm2 - v':'m2 - v', 'uvm2 - i':'m2 - i'})\n",
    "\n",
    "#Recalculate the number of bumps for this sample and save it as a column: \n",
    "filters = ['w2','m2','w1','u','b','v','i']\n",
    "n_bumps = []\n",
    "for ind,row in df.iterrows():\n",
    "    # Get the sed\n",
    "    sed = row[filters]\n",
    "    # Take the derivatives         \n",
    "    div = np.abs(np.diff(sed))\n",
    "    # Compare to allowed differences \n",
    "    compare = [not_in_bin(row,f1,f2) for f1,f2 in zip(filters[:-1],filters[1:])]\n",
    "    # Where do the bumps occur? \n",
    "    loc = np.where(compare)[0]\n",
    "    # How many bumps are there?\n",
    "    n_bumps.append(len(loc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc595112",
   "metadata": {},
   "source": [
    "#### Add cut column and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58c1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649 stars remaining\n",
      "8704 stars cut\n",
      "galaxy\n",
      "lmc    2790\n",
      "smc    1859\n",
      "Name: count, dtype: int64\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Do any cuts overlap?\n",
    "drop_index = [bad_filter_index , flat_jump_index , mismatch_index , red_index , variable_index , bad_i_index]\n",
    "drop_name = ['bad_filter','flat_jump','mismatch','red','variable','bad_i']\n",
    "\n",
    "# Re-Read in magnitude reduced files \n",
    "df = pd.read_csv(directory+'1_magnitude_reduced.csv') \n",
    "\n",
    "# Make a key\n",
    "df['key'] = np.arange(df.shape[0])\n",
    "\n",
    "# Add the number of bumps to the dataframe\n",
    "df['n_bumps'] = n_bumps\n",
    "\n",
    "\n",
    "# Make a cut column \n",
    "df['cut'] = ''\n",
    "\n",
    "# Fill the cut column \n",
    "for index, name in zip(drop_index,drop_name):\n",
    "    df.loc[df['key'].isin(index),'cut'] = name\n",
    "\n",
    "print(f'{df[df.cut == \"\"].shape[0]} stars remaining')\n",
    "print(f'{df[df.cut != \"\"].shape[0]} stars cut')\n",
    "print(df[df.cut == \"\"].galaxy.value_counts())\n",
    "\n",
    "\n",
    "# Save the dataframe\n",
    "df.to_csv(directory+'2_sed_reduced.csv',index=False)\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a6050",
   "metadata": {},
   "source": [
    "#### (Optional) Plot by cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71943d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(cutname,directory):\n",
    "    pdf = df[df.cut == cutname].copy().reset_index(drop=True)\n",
    "    n_plots = np.ceil(pdf.shape[0] / 20)\n",
    "    sed_columns = ['uvw2_mag_dered','uvm2_mag_dered','uvw1_mag_dered','U_dered','B_dered','V_dered','I_dered']\n",
    "    err_columns = ['uvw2_mag_err','uvm2_mag_err','uvw1_mag_err','e_U','e_B','e_V','e_I']\n",
    "    #Approximate central wavelengths in angstroms\n",
    "    eff_wav = [1928,2246,2600,3639.3,4350.6,5369.6,7609.2] \n",
    "\n",
    "    start = 0\n",
    "    stop = 20\n",
    "    for i in np.arange(n_plots):\n",
    "        f, axes = plt.subplots(5, 4, figsize=(20, 20))\n",
    "        for ax, n in zip(axes.flatten(),np.arange(start,stop)):\n",
    "            if n >= pdf.shape[0]:\n",
    "                break\n",
    "            row = pdf.iloc[n]\n",
    "            sed = row[sed_columns].values\n",
    "            err = row[err_columns].values\n",
    "            ax.plot(eff_wav,sed,ls='--',color='black',alpha=0.5)\n",
    "            ax.errorbar(eff_wav,sed,yerr=err,fmt='o',color='red')\n",
    "            ax.set_xticks(eff_wav)\n",
    "            ax.set_xticklabels(['W2','M2','W1','U','B','V','I'])\n",
    "            ax.set_ylim(20,14)\n",
    "            ax.grid(alpha=0.4)\n",
    "            ax.set_title(f'{row.key} - {row.cut}')\n",
    "\n",
    "        plt.savefig(f'{directory}{int(i)}.png',bbox_inches='tight')\n",
    "        plt.close()\n",
    "        start += 20\n",
    "        stop += 20\n",
    "\n",
    "make_plots = False\n",
    "if make_plots:\n",
    "    for name in drop_name:\n",
    "        print(name)\n",
    "        plot(name,f'Plots/Cuts/{name}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
